+-----------------+-------------------+
|    Parameter    |       Value       |
+=================+===================+
| Batch size      | 128               |
+-----------------+-------------------+
| Dataset         | PROTEINS          |
+-----------------+-------------------+
| Dropout ratio   | 0.5               |
+-----------------+-------------------+
| Epochs          | 10000             |
+-----------------+-------------------+
| Exp name        | PROTEINS_Glo      |
+-----------------+-------------------+
| Gpu index       | 0                 |
+-----------------+-------------------+
| Hid             | 128               |
+-----------------+-------------------+
| Lr              | 0.0005            |
+-----------------+-------------------+
| Model           | ASAPooling_Global |
+-----------------+-------------------+
| Patience        | 40                |
+-----------------+-------------------+
| Pooling ratio   | 0.5               |
+-----------------+-------------------+
| Seed            | 16                |
+-----------------+-------------------+
| Test batch size | 1                 |
+-----------------+-------------------+
| Weight decay    | 0.0001            |
+-----------------+-------------------+
Using GPU: 0
ASAPooling_Global(
  (conv1): GCNConv(3, 128)
  (conv2): GCNConv(128, 128)
  (conv3): GCNConv(128, 128)
  (pool): ASAPooling(384, ratio=0.5)
  (lin1): Linear(in_features=768, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Model Parameter: 437958
Using Adam

Epoch #000, Train_Loss: [0.6949, 0.6928, 0.6901, 0.6899, 0.6888, 0.6890, 0.6814]
Val_Loss: 0.682020, Val_Acc: 0.585586
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #001, Train_Loss: [0.6788, 0.6894, 0.6842, 0.6897, 0.6680, 0.6625, 0.6658]
Val_Loss: 0.672176, Val_Acc: 0.585586
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #002, Train_Loss: [0.6690, 0.6547, 0.6770, 0.6596, 0.6905, 0.6853, 0.6645]
Val_Loss: 0.669045, Val_Acc: 0.585586
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #003, Train_Loss: [0.6656, 0.6897, 0.6505, 0.6934, 0.6510, 0.6741, 0.6565]
Val_Loss: 0.662846, Val_Acc: 0.585586
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #004, Train_Loss: [0.6656, 0.6592, 0.6462, 0.6636, 0.6714, 0.6641, 0.6779]
Val_Loss: 0.655490, Val_Acc: 0.585586
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #005, Train_Loss: [0.6497, 0.6197, 0.6418, 0.6588, 0.6901, 0.6696, 0.6633]
Val_Loss: 0.642016, Val_Acc: 0.585586
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #006, Train_Loss: [0.6533, 0.6189, 0.6300, 0.6517, 0.6587, 0.6478, 0.6600]
Val_Loss: 0.621794, Val_Acc: 0.585586
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #007, Train_Loss: [0.6343, 0.6248, 0.6256, 0.6276, 0.6602, 0.6204, 0.6296]
Val_Loss: 0.594426, Val_Acc: 0.639640
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #008, Train_Loss: [0.6084, 0.6209, 0.5770, 0.6135, 0.6364, 0.6373, 0.5869]
Val_Loss: 0.566015, Val_Acc: 0.720721
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #009, Train_Loss: [0.6125, 0.5637, 0.6199, 0.6157, 0.6250, 0.5759, 0.5548]
Val_Loss: 0.534424, Val_Acc: 0.765766
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #010, Train_Loss: [0.5430, 0.6050, 0.6182, 0.5307, 0.6015, 0.5904, 0.5913]
Val_Loss: 0.518822, Val_Acc: 0.765766
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #011, Train_Loss: [0.5942, 0.5621, 0.5539, 0.5644, 0.5886, 0.5751, 0.5666]
Val_Loss: 0.508099, Val_Acc: 0.756757
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #012, Train_Loss: [0.5352, 0.5570, 0.5583, 0.5172, 0.6549, 0.6103, 0.5588]
Val_Loss: 0.503589, Val_Acc: 0.765766
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #013, Train_Loss: [0.6218, 0.5362, 0.5847, 0.5792, 0.5425, 0.5965, 0.5668]
Val_Loss: 0.512416, Val_Acc: 0.738739

Epoch #014, Train_Loss: [0.5848, 0.5612, 0.5349, 0.5672, 0.5114, 0.6144, 0.5120]
Val_Loss: 0.505182, Val_Acc: 0.747748

Epoch #015, Train_Loss: [0.4977, 0.5590, 0.5350, 0.5874, 0.6515, 0.5323, 0.5672]
Val_Loss: 0.517207, Val_Acc: 0.756757

Epoch #016, Train_Loss: [0.5508, 0.5298, 0.5874, 0.5756, 0.5808, 0.5870, 0.5587]
Val_Loss: 0.522039, Val_Acc: 0.729730

Epoch #017, Train_Loss: [0.5562, 0.5502, 0.5427, 0.4982, 0.5672, 0.5594, 0.6206]
Val_Loss: 0.517543, Val_Acc: 0.765766

Epoch #018, Train_Loss: [0.5825, 0.5082, 0.5388, 0.5900, 0.5464, 0.5644, 0.6024]
Val_Loss: 0.524025, Val_Acc: 0.720721

Epoch #019, Train_Loss: [0.5568, 0.4988, 0.5646, 0.5502, 0.6135, 0.5422, 0.5894]
Val_Loss: 0.519899, Val_Acc: 0.765766

Epoch #020, Train_Loss: [0.5635, 0.5374, 0.5972, 0.5316, 0.5868, 0.5369, 0.5826]
Val_Loss: 0.532780, Val_Acc: 0.711712

Epoch #021, Train_Loss: [0.5360, 0.5890, 0.5562, 0.5601, 0.5061, 0.5695, 0.5277]
Val_Loss: 0.524285, Val_Acc: 0.774775

Epoch #022, Train_Loss: [0.5375, 0.6215, 0.5919, 0.4907, 0.5958, 0.5793, 0.4522]
Val_Loss: 0.526818, Val_Acc: 0.711712

Epoch #023, Train_Loss: [0.6134, 0.5068, 0.6133, 0.5381, 0.5099, 0.5443, 0.5705]
Val_Loss: 0.517321, Val_Acc: 0.720721

Epoch #024, Train_Loss: [0.5130, 0.5673, 0.5517, 0.5833, 0.5336, 0.5450, 0.5520]
Val_Loss: 0.516372, Val_Acc: 0.756757

Epoch #025, Train_Loss: [0.4906, 0.5293, 0.5885, 0.5800, 0.5822, 0.5496, 0.4969]
Val_Loss: 0.525721, Val_Acc: 0.720721

Epoch #026, Train_Loss: [0.5536, 0.5069, 0.4993, 0.5765, 0.5563, 0.5922, 0.5940]
Val_Loss: 0.516811, Val_Acc: 0.720721

Epoch #027, Train_Loss: [0.5345, 0.4903, 0.5668, 0.5300, 0.5272, 0.5675, 0.5719]
Val_Loss: 0.523388, Val_Acc: 0.747748

Epoch #028, Train_Loss: [0.6065, 0.4607, 0.5815, 0.5133, 0.5574, 0.5456, 0.5909]
Val_Loss: 0.527982, Val_Acc: 0.702703

Epoch #029, Train_Loss: [0.5222, 0.6082, 0.5272, 0.5399, 0.5481, 0.5298, 0.5436]
Val_Loss: 0.523395, Val_Acc: 0.738739

Epoch #030, Train_Loss: [0.5365, 0.5845, 0.5615, 0.5235, 0.5004, 0.5322, 0.5576]
Val_Loss: 0.515229, Val_Acc: 0.720721

Epoch #031, Train_Loss: [0.4720, 0.5485, 0.5471, 0.5099, 0.5966, 0.5072, 0.6175]
Val_Loss: 0.528443, Val_Acc: 0.720721

Epoch #032, Train_Loss: [0.6315, 0.4941, 0.6013, 0.5038, 0.5351, 0.4922, 0.5780]
Val_Loss: 0.528954, Val_Acc: 0.756757

Epoch #033, Train_Loss: [0.4611, 0.5809, 0.6354, 0.5821, 0.5669, 0.4898, 0.5598]
Val_Loss: 0.540446, Val_Acc: 0.693694

Epoch #034, Train_Loss: [0.5801, 0.5554, 0.5449, 0.5547, 0.5623, 0.5103, 0.5310]
Val_Loss: 0.533993, Val_Acc: 0.756757

Epoch #035, Train_Loss: [0.5630, 0.5731, 0.5913, 0.5084, 0.4840, 0.5544, 0.5198]
Val_Loss: 0.516188, Val_Acc: 0.738739

Epoch #036, Train_Loss: [0.5303, 0.5384, 0.5533, 0.5435, 0.5213, 0.5211, 0.5324]
Val_Loss: 0.515885, Val_Acc: 0.720721

Epoch #037, Train_Loss: [0.5671, 0.4826, 0.5525, 0.5195, 0.5963, 0.5367, 0.5060]
Val_Loss: 0.522797, Val_Acc: 0.720721

Epoch #038, Train_Loss: [0.6361, 0.5234, 0.5291, 0.5656, 0.5721, 0.5848, 0.5337]
Val_Loss: 0.504884, Val_Acc: 0.783784

Epoch #039, Train_Loss: [0.6204, 0.5173, 0.5823, 0.5101, 0.5554, 0.5122, 0.5544]
Val_Loss: 0.510414, Val_Acc: 0.792793

Epoch #040, Train_Loss: [0.5276, 0.5530, 0.5288, 0.5348, 0.5745, 0.5673, 0.5375]
Val_Loss: 0.528548, Val_Acc: 0.774775

Epoch #041, Train_Loss: [0.5323, 0.5484, 0.5543, 0.4710, 0.5718, 0.5072, 0.6290]
Val_Loss: 0.510103, Val_Acc: 0.747748

Epoch #042, Train_Loss: [0.5221, 0.4935, 0.5084, 0.6075, 0.5975, 0.5174, 0.5424]
Val_Loss: 0.506970, Val_Acc: 0.747748

Epoch #043, Train_Loss: [0.6017, 0.5762, 0.5306, 0.5021, 0.5605, 0.4954, 0.5069]
Val_Loss: 0.505218, Val_Acc: 0.729730

Epoch #044, Train_Loss: [0.5662, 0.4951, 0.5353, 0.5225, 0.5457, 0.5393, 0.5439]
Val_Loss: 0.520936, Val_Acc: 0.747748

Epoch #045, Train_Loss: [0.5422, 0.4726, 0.6147, 0.5455, 0.5512, 0.5566, 0.5104]
Val_Loss: 0.525150, Val_Acc: 0.729730

Epoch #046, Train_Loss: [0.5092, 0.6057, 0.5795, 0.5231, 0.4702, 0.5317, 0.6170]
Val_Loss: 0.514481, Val_Acc: 0.756757

Epoch #047, Train_Loss: [0.5818, 0.5277, 0.5486, 0.5175, 0.5447, 0.4980, 0.6016]
Val_Loss: 0.514355, Val_Acc: 0.774775

Epoch #048, Train_Loss: [0.5599, 0.5775, 0.5515, 0.5229, 0.5726, 0.5364, 0.5054]
Val_Loss: 0.526406, Val_Acc: 0.720721

Epoch #049, Train_Loss: [0.5700, 0.5362, 0.4820, 0.5328, 0.5705, 0.5367, 0.5765]
Val_Loss: 0.502210, Val_Acc: 0.729730
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #050, Train_Loss: [0.6067, 0.4962, 0.4985, 0.4910, 0.5622, 0.5801, 0.5058]
Val_Loss: 0.511588, Val_Acc: 0.738739

Epoch #051, Train_Loss: [0.4664, 0.5890, 0.5003, 0.5631, 0.5714, 0.5319, 0.5828]
Val_Loss: 0.521595, Val_Acc: 0.738739

Epoch #052, Train_Loss: [0.5407, 0.5023, 0.5655, 0.5536, 0.5305, 0.5088, 0.5501]
Val_Loss: 0.512266, Val_Acc: 0.711712

Epoch #053, Train_Loss: [0.5480, 0.5458, 0.5539, 0.5707, 0.5938, 0.5085, 0.4412]
Val_Loss: 0.518413, Val_Acc: 0.756757

Epoch #054, Train_Loss: [0.5320, 0.5093, 0.4729, 0.5879, 0.5508, 0.5723, 0.5370]
Val_Loss: 0.525581, Val_Acc: 0.720721

Epoch #055, Train_Loss: [0.4955, 0.5436, 0.5144, 0.5905, 0.5259, 0.5735, 0.5410]
Val_Loss: 0.513873, Val_Acc: 0.729730

Epoch #056, Train_Loss: [0.4973, 0.5199, 0.6096, 0.5749, 0.5326, 0.5188, 0.5382]
Val_Loss: 0.511818, Val_Acc: 0.729730

Epoch #057, Train_Loss: [0.4995, 0.5525, 0.4183, 0.5774, 0.4869, 0.6841, 0.5324]
Val_Loss: 0.511302, Val_Acc: 0.729730

Epoch #058, Train_Loss: [0.5378, 0.5462, 0.4895, 0.5534, 0.4936, 0.5491, 0.5461]
Val_Loss: 0.520258, Val_Acc: 0.738739

Epoch #059, Train_Loss: [0.5048, 0.4998, 0.4400, 0.6091, 0.5952, 0.4965, 0.5602]
Val_Loss: 0.527592, Val_Acc: 0.702703

Epoch #060, Train_Loss: [0.5334, 0.5869, 0.5522, 0.5432, 0.5677, 0.4754, 0.5305]
Val_Loss: 0.509634, Val_Acc: 0.729730

Epoch #061, Train_Loss: [0.5468, 0.5673, 0.4894, 0.5272, 0.6099, 0.5127, 0.5097]
Val_Loss: 0.524213, Val_Acc: 0.720721

Epoch #062, Train_Loss: [0.5186, 0.5284, 0.5019, 0.5415, 0.6128, 0.5388, 0.4895]
Val_Loss: 0.514922, Val_Acc: 0.756757

Epoch #063, Train_Loss: [0.6027, 0.4817, 0.5040, 0.5319, 0.5247, 0.5217, 0.5538]
Val_Loss: 0.524899, Val_Acc: 0.720721

Epoch #064, Train_Loss: [0.4997, 0.5489, 0.5170, 0.4979, 0.5735, 0.5298, 0.5941]
Val_Loss: 0.526069, Val_Acc: 0.711712

Epoch #065, Train_Loss: [0.5202, 0.4802, 0.5633, 0.5244, 0.5594, 0.6136, 0.5233]
Val_Loss: 0.509695, Val_Acc: 0.711712

Epoch #066, Train_Loss: [0.5382, 0.4949, 0.5215, 0.5951, 0.5435, 0.4912, 0.5032]
Val_Loss: 0.516903, Val_Acc: 0.720721

Epoch #067, Train_Loss: [0.4943, 0.5555, 0.5548, 0.5818, 0.4927, 0.4815, 0.5837]
Val_Loss: 0.510026, Val_Acc: 0.756757

Epoch #068, Train_Loss: [0.4923, 0.5689, 0.5468, 0.5549, 0.5127, 0.5465, 0.5142]
Val_Loss: 0.511610, Val_Acc: 0.738739

Epoch #069, Train_Loss: [0.5823, 0.4625, 0.4857, 0.5350, 0.5307, 0.5754, 0.5501]
Val_Loss: 0.508617, Val_Acc: 0.756757

Epoch #070, Train_Loss: [0.5676, 0.5616, 0.5899, 0.5128, 0.5264, 0.5047, 0.4916]
Val_Loss: 0.524075, Val_Acc: 0.738739

Epoch #071, Train_Loss: [0.4980, 0.5663, 0.5532, 0.5370, 0.5187, 0.4772, 0.5423]
Val_Loss: 0.507314, Val_Acc: 0.774775

Epoch #072, Train_Loss: [0.5246, 0.5357, 0.5884, 0.4651, 0.4636, 0.5214, 0.5920]
Val_Loss: 0.514871, Val_Acc: 0.738739

Epoch #073, Train_Loss: [0.5349, 0.5077, 0.5319, 0.5370, 0.5282, 0.5086, 0.5685]
Val_Loss: 0.505766, Val_Acc: 0.747748

Epoch #074, Train_Loss: [0.5482, 0.5351, 0.5764, 0.4780, 0.5624, 0.5145, 0.4748]
Val_Loss: 0.517307, Val_Acc: 0.738739

Epoch #075, Train_Loss: [0.5793, 0.5088, 0.5341, 0.4978, 0.5348, 0.5209, 0.5132]
Val_Loss: 0.505593, Val_Acc: 0.747748

Epoch #076, Train_Loss: [0.4832, 0.5386, 0.5472, 0.5492, 0.4901, 0.5370, 0.5105]
Val_Loss: 0.513819, Val_Acc: 0.720721

Epoch #077, Train_Loss: [0.5265, 0.5210, 0.4834, 0.5824, 0.6104, 0.4340, 0.4941]
Val_Loss: 0.508362, Val_Acc: 0.738739

Epoch #078, Train_Loss: [0.4920, 0.4870, 0.5429, 0.5896, 0.5114, 0.5541, 0.4640]
Val_Loss: 0.522794, Val_Acc: 0.729730

Epoch #079, Train_Loss: [0.5351, 0.5470, 0.5617, 0.4879, 0.5044, 0.4825, 0.4867]
Val_Loss: 0.508991, Val_Acc: 0.783784

Epoch #080, Train_Loss: [0.5069, 0.4858, 0.4887, 0.4965, 0.6011, 0.4949, 0.5933]
Val_Loss: 0.520490, Val_Acc: 0.720721

Epoch #081, Train_Loss: [0.4944, 0.4073, 0.4917, 0.4839, 0.6376, 0.5809, 0.5500]
Val_Loss: 0.517220, Val_Acc: 0.720721

Epoch #082, Train_Loss: [0.5269, 0.5882, 0.4887, 0.5856, 0.5265, 0.4961, 0.4630]
Val_Loss: 0.503103, Val_Acc: 0.756757

Epoch #083, Train_Loss: [0.5509, 0.4851, 0.5422, 0.6001, 0.4854, 0.4880, 0.4915]
Val_Loss: 0.509153, Val_Acc: 0.729730

Epoch #084, Train_Loss: [0.4264, 0.5034, 0.5040, 0.4951, 0.5669, 0.6119, 0.5043]
Val_Loss: 0.501585, Val_Acc: 0.756757
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #085, Train_Loss: [0.5126, 0.5578, 0.5297, 0.4499, 0.5467, 0.4669, 0.5353]
Val_Loss: 0.503887, Val_Acc: 0.756757

Epoch #086, Train_Loss: [0.5718, 0.4414, 0.5285, 0.5341, 0.5164, 0.5286, 0.5526]
Val_Loss: 0.514239, Val_Acc: 0.738739

Epoch #087, Train_Loss: [0.5351, 0.5461, 0.5094, 0.4695, 0.5805, 0.4593, 0.5533]
Val_Loss: 0.515387, Val_Acc: 0.783784

Epoch #088, Train_Loss: [0.5079, 0.5576, 0.5483, 0.4329, 0.4730, 0.5722, 0.5560]
Val_Loss: 0.515858, Val_Acc: 0.783784

Epoch #089, Train_Loss: [0.5241, 0.5797, 0.4329, 0.5884, 0.5345, 0.4622, 0.5000]
Val_Loss: 0.497550, Val_Acc: 0.792793
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #090, Train_Loss: [0.4823, 0.4672, 0.5844, 0.4663, 0.4589, 0.5582, 0.5666]
Val_Loss: 0.493720, Val_Acc: 0.765766
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #091, Train_Loss: [0.5091, 0.5185, 0.4894, 0.4415, 0.5701, 0.5770, 0.5306]
Val_Loss: 0.502293, Val_Acc: 0.765766

Epoch #092, Train_Loss: [0.4618, 0.4602, 0.5823, 0.5278, 0.4869, 0.5571, 0.5334]
Val_Loss: 0.520584, Val_Acc: 0.747748

Epoch #093, Train_Loss: [0.4565, 0.5284, 0.5169, 0.5089, 0.5117, 0.5446, 0.5160]
Val_Loss: 0.507226, Val_Acc: 0.783784

Epoch #094, Train_Loss: [0.5691, 0.4865, 0.5886, 0.5068, 0.5230, 0.4501, 0.5031]
Val_Loss: 0.515494, Val_Acc: 0.774775

Epoch #095, Train_Loss: [0.4989, 0.5160, 0.4948, 0.6229, 0.5300, 0.4796, 0.4786]
Val_Loss: 0.524645, Val_Acc: 0.729730

Epoch #096, Train_Loss: [0.4795, 0.5419, 0.5348, 0.5483, 0.5102, 0.4288, 0.5828]
Val_Loss: 0.509696, Val_Acc: 0.756757

Epoch #097, Train_Loss: [0.4548, 0.5657, 0.5281, 0.5337, 0.4005, 0.5693, 0.5326]
Val_Loss: 0.492484, Val_Acc: 0.756757
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #098, Train_Loss: [0.4633, 0.6040, 0.4776, 0.4639, 0.5541, 0.5580, 0.5441]
Val_Loss: 0.501782, Val_Acc: 0.765766

Epoch #099, Train_Loss: [0.4546, 0.5355, 0.5190, 0.5249, 0.5518, 0.5406, 0.5067]
Val_Loss: 0.519796, Val_Acc: 0.720721

Epoch #100, Train_Loss: [0.4883, 0.5483, 0.4886, 0.4717, 0.5299, 0.5726, 0.5237]
Val_Loss: 0.515526, Val_Acc: 0.765766

Epoch #101, Train_Loss: [0.5196, 0.4876, 0.4636, 0.4127, 0.5207, 0.6968, 0.5156]
Val_Loss: 0.512879, Val_Acc: 0.756757

Epoch #102, Train_Loss: [0.4931, 0.5298, 0.5346, 0.5059, 0.5363, 0.5055, 0.5236]
Val_Loss: 0.505117, Val_Acc: 0.720721

Epoch #103, Train_Loss: [0.5969, 0.4743, 0.4703, 0.5379, 0.5004, 0.4892, 0.5104]
Val_Loss: 0.480023, Val_Acc: 0.801802
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #104, Train_Loss: [0.4567, 0.5774, 0.5127, 0.5668, 0.4984, 0.5335, 0.4529]
Val_Loss: 0.502533, Val_Acc: 0.756757

Epoch #105, Train_Loss: [0.4965, 0.5280, 0.5720, 0.5131, 0.4671, 0.5123, 0.4951]
Val_Loss: 0.488329, Val_Acc: 0.801802

Epoch #106, Train_Loss: [0.5346, 0.5404, 0.4372, 0.5667, 0.5289, 0.5333, 0.4529]
Val_Loss: 0.506915, Val_Acc: 0.765766

Epoch #107, Train_Loss: [0.4743, 0.5449, 0.5342, 0.3524, 0.5331, 0.6104, 0.5391]
Val_Loss: 0.477558, Val_Acc: 0.792793
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #108, Train_Loss: [0.5237, 0.5071, 0.5169, 0.5194, 0.4871, 0.4708, 0.5388]
Val_Loss: 0.473365, Val_Acc: 0.792793
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #109, Train_Loss: [0.5759, 0.4197, 0.5560, 0.4809, 0.5849, 0.4829, 0.4757]
Val_Loss: 0.485301, Val_Acc: 0.783784

Epoch #110, Train_Loss: [0.4486, 0.4369, 0.5255, 0.5386, 0.5408, 0.5304, 0.5672]
Val_Loss: 0.503612, Val_Acc: 0.783784

Epoch #111, Train_Loss: [0.5204, 0.4983, 0.5149, 0.4866, 0.4959, 0.4864, 0.5285]
Val_Loss: 0.486413, Val_Acc: 0.792793

Epoch #112, Train_Loss: [0.5789, 0.5215, 0.4445, 0.5336, 0.5366, 0.4353, 0.4984]
Val_Loss: 0.488210, Val_Acc: 0.774775

Epoch #113, Train_Loss: [0.5685, 0.4696, 0.4753, 0.4793, 0.4587, 0.5349, 0.5825]
Val_Loss: 0.466036, Val_Acc: 0.819820
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #114, Train_Loss: [0.5680, 0.5282, 0.5419, 0.4233, 0.4850, 0.4874, 0.4763]
Val_Loss: 0.474525, Val_Acc: 0.801802

Epoch #115, Train_Loss: [0.4811, 0.5130, 0.5068, 0.5118, 0.4894, 0.5009, 0.4793]
Val_Loss: 0.474696, Val_Acc: 0.819820

Epoch #116, Train_Loss: [0.4524, 0.5104, 0.5502, 0.4924, 0.4682, 0.4817, 0.5033]
Val_Loss: 0.470383, Val_Acc: 0.810811

Epoch #117, Train_Loss: [0.4716, 0.5177, 0.5605, 0.5118, 0.5209, 0.4495, 0.4599]
Val_Loss: 0.465884, Val_Acc: 0.801802
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #118, Train_Loss: [0.5215, 0.4883, 0.4696, 0.5247, 0.5093, 0.4712, 0.4926]
Val_Loss: 0.459522, Val_Acc: 0.792793
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #119, Train_Loss: [0.4746, 0.4614, 0.5004, 0.5040, 0.5343, 0.4701, 0.5244]
Val_Loss: 0.473836, Val_Acc: 0.801802

Epoch #120, Train_Loss: [0.4828, 0.4820, 0.5133, 0.4521, 0.4942, 0.5025, 0.5363]
Val_Loss: 0.470192, Val_Acc: 0.819820

Epoch #121, Train_Loss: [0.4868, 0.4736, 0.5137, 0.5096, 0.5034, 0.5214, 0.4915]
Val_Loss: 0.470278, Val_Acc: 0.792793

Epoch #122, Train_Loss: [0.5234, 0.5367, 0.4409, 0.5016, 0.5129, 0.4564, 0.4836]
Val_Loss: 0.467182, Val_Acc: 0.783784

Epoch #123, Train_Loss: [0.4637, 0.4946, 0.4680, 0.4285, 0.5342, 0.5770, 0.5317]
Val_Loss: 0.480465, Val_Acc: 0.783784

Epoch #124, Train_Loss: [0.5150, 0.4830, 0.4614, 0.4475, 0.4870, 0.5882, 0.4942]
Val_Loss: 0.469444, Val_Acc: 0.801802

Epoch #125, Train_Loss: [0.5673, 0.4509, 0.4537, 0.4412, 0.4555, 0.4709, 0.6050]
Val_Loss: 0.472557, Val_Acc: 0.792793

Epoch #126, Train_Loss: [0.5110, 0.5369, 0.4529, 0.5117, 0.4692, 0.4743, 0.5584]
Val_Loss: 0.495669, Val_Acc: 0.792793

Epoch #127, Train_Loss: [0.4342, 0.4625, 0.5156, 0.5382, 0.4744, 0.4946, 0.5652]
Val_Loss: 0.492102, Val_Acc: 0.792793

Epoch #128, Train_Loss: [0.5007, 0.4688, 0.4661, 0.4483, 0.5792, 0.4590, 0.5257]
Val_Loss: 0.473015, Val_Acc: 0.801802

Epoch #129, Train_Loss: [0.4914, 0.5455, 0.4959, 0.4333, 0.5398, 0.5309, 0.4705]
Val_Loss: 0.472053, Val_Acc: 0.792793

Epoch #130, Train_Loss: [0.5655, 0.4679, 0.5065, 0.5300, 0.5067, 0.5034, 0.4200]
Val_Loss: 0.481587, Val_Acc: 0.792793

Epoch #131, Train_Loss: [0.5227, 0.3998, 0.5145, 0.5336, 0.5472, 0.4630, 0.4382]
Val_Loss: 0.468753, Val_Acc: 0.810811

Epoch #132, Train_Loss: [0.4678, 0.4799, 0.4898, 0.4601, 0.5080, 0.5077, 0.5265]
Val_Loss: 0.467432, Val_Acc: 0.810811

Epoch #133, Train_Loss: [0.4903, 0.4675, 0.5381, 0.4987, 0.4700, 0.4984, 0.5167]
Val_Loss: 0.467958, Val_Acc: 0.828829

Epoch #134, Train_Loss: [0.5206, 0.5292, 0.4998, 0.4397, 0.4681, 0.4610, 0.5015]
Val_Loss: 0.467415, Val_Acc: 0.810811

Epoch #135, Train_Loss: [0.4872, 0.5088, 0.4627, 0.5284, 0.4283, 0.4764, 0.5347]
Val_Loss: 0.476534, Val_Acc: 0.810811

Epoch #136, Train_Loss: [0.5478, 0.4961, 0.4669, 0.4720, 0.4314, 0.4963, 0.5129]
Val_Loss: 0.461285, Val_Acc: 0.837838

Epoch #137, Train_Loss: [0.5065, 0.4049, 0.4626, 0.4938, 0.4936, 0.5452, 0.5226]
Val_Loss: 0.467890, Val_Acc: 0.819820

Epoch #138, Train_Loss: [0.4620, 0.4746, 0.5371, 0.5276, 0.4851, 0.4887, 0.4683]
Val_Loss: 0.472767, Val_Acc: 0.801802

Epoch #139, Train_Loss: [0.4254, 0.4697, 0.4846, 0.5989, 0.4738, 0.5383, 0.4511]
Val_Loss: 0.481065, Val_Acc: 0.792793

Epoch #140, Train_Loss: [0.4995, 0.4947, 0.4958, 0.4473, 0.5459, 0.4919, 0.4382]
Val_Loss: 0.470580, Val_Acc: 0.819820

Epoch #141, Train_Loss: [0.5035, 0.5345, 0.4106, 0.4458, 0.4789, 0.4585, 0.5307]
Val_Loss: 0.469886, Val_Acc: 0.819820

Epoch #142, Train_Loss: [0.5161, 0.3776, 0.4435, 0.3958, 0.5382, 0.5855, 0.5947]
Val_Loss: 0.471018, Val_Acc: 0.801802

Epoch #143, Train_Loss: [0.4754, 0.5139, 0.4713, 0.4966, 0.4799, 0.4669, 0.5072]
Val_Loss: 0.474298, Val_Acc: 0.837838

Epoch #144, Train_Loss: [0.5308, 0.5775, 0.4167, 0.5192, 0.4121, 0.4611, 0.5005]
Val_Loss: 0.469062, Val_Acc: 0.828829

Epoch #145, Train_Loss: [0.4965, 0.5093, 0.4257, 0.5581, 0.4686, 0.5143, 0.4445]
Val_Loss: 0.473628, Val_Acc: 0.837838

Epoch #146, Train_Loss: [0.5131, 0.4545, 0.4759, 0.4637, 0.4925, 0.5040, 0.4943]
Val_Loss: 0.470542, Val_Acc: 0.828829

Epoch #147, Train_Loss: [0.5065, 0.4773, 0.4821, 0.4853, 0.5130, 0.4751, 0.4838]
Val_Loss: 0.463669, Val_Acc: 0.828829

Epoch #148, Train_Loss: [0.4458, 0.4796, 0.5278, 0.4719, 0.5105, 0.4983, 0.4749]
Val_Loss: 0.461871, Val_Acc: 0.828829

Epoch #149, Train_Loss: [0.4434, 0.4923, 0.5283, 0.5251, 0.4494, 0.4862, 0.4558]
Val_Loss: 0.463437, Val_Acc: 0.828829

Epoch #150, Train_Loss: [0.5372, 0.4706, 0.4809, 0.4306, 0.4385, 0.4957, 0.5136]
Val_Loss: 0.461930, Val_Acc: 0.828829

Epoch #151, Train_Loss: [0.4975, 0.5560, 0.4668, 0.5317, 0.4666, 0.4424, 0.4929]
Val_Loss: 0.467395, Val_Acc: 0.846847

Epoch #152, Train_Loss: [0.4709, 0.4237, 0.5387, 0.4355, 0.5919, 0.4564, 0.4596]
Val_Loss: 0.468424, Val_Acc: 0.837838

Epoch #153, Train_Loss: [0.4901, 0.4084, 0.4728, 0.3963, 0.5433, 0.5021, 0.5332]
Val_Loss: 0.461855, Val_Acc: 0.828829

Epoch #154, Train_Loss: [0.5226, 0.4843, 0.4348, 0.4142, 0.4569, 0.5621, 0.4994]
Val_Loss: 0.472750, Val_Acc: 0.810811

Epoch #155, Train_Loss: [0.5667, 0.5546, 0.4581, 0.4588, 0.4380, 0.3945, 0.5048]
Val_Loss: 0.475868, Val_Acc: 0.783784

Epoch #156, Train_Loss: [0.4964, 0.4482, 0.4717, 0.5437, 0.4652, 0.4202, 0.5340]
Val_Loss: 0.474724, Val_Acc: 0.810811

Epoch #157, Train_Loss: [0.4509, 0.4370, 0.5631, 0.5041, 0.4895, 0.4318, 0.4747]
Val_Loss: 0.465122, Val_Acc: 0.828829

Epoch #158, Train_Loss: [0.4441, 0.5733, 0.4352, 0.4292, 0.4504, 0.5224, 0.5318]
Val_Loss: 0.459369, Val_Acc: 0.837838
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********

Epoch #159, Train_Loss: [0.5411, 0.4334, 0.4960, 0.4717, 0.4920, 0.4458, 0.4553]
Val_Loss: 0.489472, Val_Acc: 0.801802

Epoch #160, Train_Loss: [0.4892, 0.4605, 0.4907, 0.5215, 0.5022, 0.4802, 0.4412]
Val_Loss: 0.472658, Val_Acc: 0.819820

Epoch #161, Train_Loss: [0.4780, 0.4908, 0.4117, 0.4331, 0.4885, 0.5031, 0.5389]
Val_Loss: 0.474759, Val_Acc: 0.810811

Epoch #162, Train_Loss: [0.5052, 0.5298, 0.4755, 0.4628, 0.4707, 0.4420, 0.4537]
Val_Loss: 0.468516, Val_Acc: 0.819820

Epoch #163, Train_Loss: [0.4382, 0.5726, 0.4271, 0.4717, 0.5060, 0.4392, 0.5055]
Val_Loss: 0.483544, Val_Acc: 0.810811

Epoch #164, Train_Loss: [0.4741, 0.5092, 0.5432, 0.5125, 0.4143, 0.5171, 0.4224]
Val_Loss: 0.481054, Val_Acc: 0.819820

Epoch #165, Train_Loss: [0.4486, 0.4657, 0.4180, 0.4205, 0.5418, 0.5124, 0.4918]
Val_Loss: 0.483438, Val_Acc: 0.810811

Epoch #166, Train_Loss: [0.4639, 0.4217, 0.4994, 0.5601, 0.4191, 0.4725, 0.4846]
Val_Loss: 0.479113, Val_Acc: 0.810811

Epoch #167, Train_Loss: [0.5335, 0.4258, 0.4273, 0.4965, 0.5191, 0.4659, 0.4141]
Val_Loss: 0.480413, Val_Acc: 0.801802

Epoch #168, Train_Loss: [0.5362, 0.4638, 0.4389, 0.5655, 0.4865, 0.4303, 0.4762]
Val_Loss: 0.485459, Val_Acc: 0.819820

Epoch #169, Train_Loss: [0.5234, 0.4231, 0.4859, 0.3975, 0.4947, 0.4702, 0.5410]
Val_Loss: 0.467229, Val_Acc: 0.837838

Epoch #170, Train_Loss: [0.4163, 0.4821, 0.5134, 0.4885, 0.4440, 0.4966, 0.4493]
Val_Loss: 0.469276, Val_Acc: 0.801802

Epoch #171, Train_Loss: [0.4188, 0.4866, 0.5172, 0.5597, 0.4303, 0.4254, 0.4745]
Val_Loss: 0.470710, Val_Acc: 0.792793

Epoch #172, Train_Loss: [0.4218, 0.4604, 0.5067, 0.4717, 0.5131, 0.4983, 0.4429]
Val_Loss: 0.474928, Val_Acc: 0.819820

Epoch #173, Train_Loss: [0.4671, 0.4666, 0.5035, 0.4329, 0.4508, 0.4895, 0.4701]
Val_Loss: 0.475065, Val_Acc: 0.810811

Epoch #174, Train_Loss: [0.4622, 0.5452, 0.4080, 0.4384, 0.5105, 0.4302, 0.4901]
Val_Loss: 0.472848, Val_Acc: 0.801802

Epoch #175, Train_Loss: [0.4699, 0.5144, 0.4428, 0.3919, 0.4721, 0.5638, 0.4530]
Val_Loss: 0.488402, Val_Acc: 0.801802

Epoch #176, Train_Loss: [0.4087, 0.5471, 0.4295, 0.5135, 0.4652, 0.5039, 0.4236]
Val_Loss: 0.488403, Val_Acc: 0.801802

Epoch #177, Train_Loss: [0.4891, 0.5000, 0.4439, 0.5238, 0.4515, 0.4684, 0.4280]
Val_Loss: 0.481339, Val_Acc: 0.792793

Epoch #178, Train_Loss: [0.4613, 0.4502, 0.5409, 0.4428, 0.4091, 0.4223, 0.5456]
Val_Loss: 0.497940, Val_Acc: 0.792793

Epoch #179, Train_Loss: [0.5040, 0.4136, 0.5197, 0.4255, 0.4366, 0.5295, 0.5168]
Val_Loss: 0.494467, Val_Acc: 0.783784

Epoch #180, Train_Loss: [0.4800, 0.4629, 0.4682, 0.4805, 0.5281, 0.4406, 0.5051]
Val_Loss: 0.493372, Val_Acc: 0.783784

Epoch #181, Train_Loss: [0.5060, 0.3576, 0.4213, 0.5912, 0.4890, 0.4537, 0.5030]
Val_Loss: 0.479205, Val_Acc: 0.810811

Epoch #182, Train_Loss: [0.5163, 0.4433, 0.5057, 0.3792, 0.4627, 0.5580, 0.4712]
Val_Loss: 0.491399, Val_Acc: 0.783784

Epoch #183, Train_Loss: [0.5347, 0.4379, 0.6016, 0.4324, 0.4574, 0.4392, 0.3942]
Val_Loss: 0.482087, Val_Acc: 0.801802

Epoch #184, Train_Loss: [0.4698, 0.5196, 0.4462, 0.4926, 0.4416, 0.3926, 0.5468]
Val_Loss: 0.482901, Val_Acc: 0.801802

Epoch #185, Train_Loss: [0.4260, 0.4919, 0.4602, 0.3956, 0.5256, 0.5197, 0.4905]
Val_Loss: 0.488217, Val_Acc: 0.801802

Epoch #186, Train_Loss: [0.5036, 0.4431, 0.4553, 0.4919, 0.4825, 0.4448, 0.4933]
Val_Loss: 0.482553, Val_Acc: 0.792793

Epoch #187, Train_Loss: [0.4617, 0.4423, 0.4450, 0.4571, 0.4878, 0.4887, 0.5385]
Val_Loss: 0.493740, Val_Acc: 0.801802

Epoch #188, Train_Loss: [0.3966, 0.4487, 0.6164, 0.4743, 0.4958, 0.4401, 0.4499]
Val_Loss: 0.485165, Val_Acc: 0.792793

Epoch #189, Train_Loss: [0.4941, 0.4886, 0.4720, 0.4686, 0.4568, 0.4584, 0.4480]
Val_Loss: 0.490637, Val_Acc: 0.774775

Epoch #190, Train_Loss: [0.4889, 0.4773, 0.4559, 0.4559, 0.4018, 0.5075, 0.5215]
Val_Loss: 0.487239, Val_Acc: 0.810811

Epoch #191, Train_Loss: [0.4476, 0.4754, 0.4611, 0.5410, 0.4671, 0.4741, 0.4255]
Val_Loss: 0.483855, Val_Acc: 0.783784

Epoch #192, Train_Loss: [0.5168, 0.5349, 0.4382, 0.5011, 0.4254, 0.3813, 0.5136]
Val_Loss: 0.506818, Val_Acc: 0.801802

Epoch #193, Train_Loss: [0.5054, 0.4647, 0.4555, 0.4864, 0.4870, 0.5180, 0.4259]
Val_Loss: 0.492883, Val_Acc: 0.783784

Epoch #194, Train_Loss: [0.4214, 0.4882, 0.4768, 0.5059, 0.4143, 0.4911, 0.4719]
Val_Loss: 0.489558, Val_Acc: 0.783784

Epoch #195, Train_Loss: [0.4959, 0.4794, 0.4842, 0.4630, 0.3697, 0.4477, 0.4973]
Val_Loss: 0.484917, Val_Acc: 0.801802

Epoch #196, Train_Loss: [0.5460, 0.4094, 0.4940, 0.4084, 0.4706, 0.4123, 0.4654]
Val_Loss: 0.493776, Val_Acc: 0.783784

Epoch #197, Train_Loss: [0.4896, 0.4718, 0.4799, 0.4968, 0.3554, 0.4890, 0.4386]
Val_Loss: 0.487654, Val_Acc: 0.801802

Epoch #198, Train_Loss: [0.5105, 0.4992, 0.4223, 0.4453, 0.4666, 0.4886, 0.4607]
Val_Loss: 0.493396, Val_Acc: 0.783784

Epoch #199, Train_Loss: [0.4332, 0.4083, 0.5023, 0.4746, 0.5677, 0.4988, 0.4470]
Val_Loss: 0.504077, Val_Acc: 0.774775

********** TEST START **********
Reload Best Model
The current best model is saved in: ******** outputs/PROTEINS_Glo/model.pth *********
TEST :: Test_Acc: 0.732143
