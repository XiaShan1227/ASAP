+-----------------+-------------------+
|    Parameter    |       Value       |
+=================+===================+
| Batch size      | 128               |
+-----------------+-------------------+
| Dataset         | DD                |
+-----------------+-------------------+
| Dropout ratio   | 0.5               |
+-----------------+-------------------+
| Epochs          | 10000             |
+-----------------+-------------------+
| Exp name        | DD_Glo            |
+-----------------+-------------------+
| Gpu index       | 0                 |
+-----------------+-------------------+
| Hid             | 128               |
+-----------------+-------------------+
| Lr              | 0.0005            |
+-----------------+-------------------+
| Model           | ASAPooling_Global |
+-----------------+-------------------+
| Patience        | 40                |
+-----------------+-------------------+
| Pooling ratio   | 0.5               |
+-----------------+-------------------+
| Seed            | 16                |
+-----------------+-------------------+
| Test batch size | 1                 |
+-----------------+-------------------+
| Weight decay    | 0.0001            |
+-----------------+-------------------+
Using GPU: 0
ASAPooling_Global(
  (conv1): GCNConv(89, 128)
  (conv2): GCNConv(128, 128)
  (conv3): GCNConv(128, 128)
  (pool): ASAPooling(384, ratio=0.5)
  (lin1): Linear(in_features=768, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Model Parameter: 448966
Using Adam

Epoch #000, Train_Loss: [0.6967, 0.6941, 0.6938, 0.6921, 0.6917, 0.6887, 0.6901, 0.6868]
Val_Loss: 0.690773, Val_Acc: 0.529915
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #001, Train_Loss: [0.6836, 0.6872, 0.6829, 0.6785, 0.6830, 0.6728, 0.6719, 0.6689]
Val_Loss: 0.690328, Val_Acc: 0.529915
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #002, Train_Loss: [0.6447, 0.6542, 0.6644, 0.6763, 0.6640, 0.6805, 0.6958, 0.7210]
Val_Loss: 0.707953, Val_Acc: 0.529915

Epoch #003, Train_Loss: [0.6677, 0.6897, 0.6742, 0.6743, 0.6589, 0.6693, 0.6588, 0.6154]
Val_Loss: 0.693318, Val_Acc: 0.529915

Epoch #004, Train_Loss: [0.6519, 0.6909, 0.6723, 0.6797, 0.6497, 0.6596, 0.6190, 0.6925]
Val_Loss: 0.694100, Val_Acc: 0.529915

Epoch #005, Train_Loss: [0.6576, 0.6716, 0.6405, 0.6791, 0.6371, 0.7024, 0.6366, 0.6758]
Val_Loss: 0.696669, Val_Acc: 0.529915

Epoch #006, Train_Loss: [0.6779, 0.6413, 0.6385, 0.6432, 0.6646, 0.6679, 0.6635, 0.6593]
Val_Loss: 0.695059, Val_Acc: 0.529915

Epoch #007, Train_Loss: [0.6460, 0.6558, 0.6587, 0.6297, 0.6571, 0.6653, 0.6766, 0.6651]
Val_Loss: 0.690801, Val_Acc: 0.529915

Epoch #008, Train_Loss: [0.6842, 0.6203, 0.6456, 0.6524, 0.6619, 0.6441, 0.6341, 0.6495]
Val_Loss: 0.689818, Val_Acc: 0.529915
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #009, Train_Loss: [0.6272, 0.6182, 0.6946, 0.6526, 0.6544, 0.6389, 0.6114, 0.6595]
Val_Loss: 0.690793, Val_Acc: 0.529915

Epoch #010, Train_Loss: [0.6710, 0.6370, 0.6420, 0.6125, 0.6312, 0.6256, 0.6319, 0.6674]
Val_Loss: 0.693651, Val_Acc: 0.529915

Epoch #011, Train_Loss: [0.5998, 0.6257, 0.6410, 0.6512, 0.6474, 0.6436, 0.6058, 0.5569]
Val_Loss: 0.688128, Val_Acc: 0.529915
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #012, Train_Loss: [0.6603, 0.6285, 0.5968, 0.5950, 0.5914, 0.6450, 0.5999, 0.6310]
Val_Loss: 0.698093, Val_Acc: 0.572650

Epoch #013, Train_Loss: [0.5951, 0.6267, 0.6310, 0.6087, 0.6155, 0.5952, 0.6097, 0.5426]
Val_Loss: 0.721110, Val_Acc: 0.606838

Epoch #014, Train_Loss: [0.6278, 0.6003, 0.6022, 0.6284, 0.5657, 0.5596, 0.6037, 0.6091]
Val_Loss: 0.702952, Val_Acc: 0.615385

Epoch #015, Train_Loss: [0.5990, 0.5964, 0.5800, 0.5953, 0.6092, 0.5534, 0.5869, 0.6036]
Val_Loss: 0.702386, Val_Acc: 0.623932

Epoch #016, Train_Loss: [0.5871, 0.5662, 0.5740, 0.5836, 0.5446, 0.5636, 0.6406, 0.5471]
Val_Loss: 0.692398, Val_Acc: 0.666667

Epoch #017, Train_Loss: [0.4958, 0.5964, 0.5818, 0.6231, 0.5764, 0.5497, 0.5938, 0.5715]
Val_Loss: 0.679487, Val_Acc: 0.666667
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #018, Train_Loss: [0.5114, 0.5469, 0.5734, 0.5111, 0.5611, 0.6061, 0.5994, 0.5805]
Val_Loss: 0.664341, Val_Acc: 0.632479
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #019, Train_Loss: [0.5880, 0.5772, 0.5422, 0.5530, 0.5583, 0.6510, 0.5035, 0.4753]
Val_Loss: 0.657559, Val_Acc: 0.692308
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #020, Train_Loss: [0.5189, 0.5399, 0.5452, 0.5216, 0.5892, 0.5351, 0.4937, 0.4519]
Val_Loss: 0.714853, Val_Acc: 0.666667

Epoch #021, Train_Loss: [0.5355, 0.5155, 0.5221, 0.5465, 0.4608, 0.5341, 0.5638, 0.6321]
Val_Loss: 0.680328, Val_Acc: 0.675214

Epoch #022, Train_Loss: [0.4899, 0.4954, 0.5816, 0.5450, 0.5240, 0.5038, 0.4927, 0.5007]
Val_Loss: 0.668023, Val_Acc: 0.675214

Epoch #023, Train_Loss: [0.4010, 0.5582, 0.5561, 0.5063, 0.4888, 0.5193, 0.5047, 0.5143]
Val_Loss: 0.626562, Val_Acc: 0.615385
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #024, Train_Loss: [0.5235, 0.4992, 0.5127, 0.4775, 0.5539, 0.4244, 0.4429, 0.5332]
Val_Loss: 0.621705, Val_Acc: 0.709402
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #025, Train_Loss: [0.4698, 0.5191, 0.4836, 0.5100, 0.4800, 0.4373, 0.4515, 0.5618]
Val_Loss: 0.662077, Val_Acc: 0.692308

Epoch #026, Train_Loss: [0.4059, 0.4994, 0.4072, 0.4463, 0.5253, 0.4546, 0.4634, 0.4345]
Val_Loss: 0.661651, Val_Acc: 0.692308

Epoch #027, Train_Loss: [0.4779, 0.4872, 0.3947, 0.4719, 0.4147, 0.4910, 0.4486, 0.3873]
Val_Loss: 0.624870, Val_Acc: 0.700855

Epoch #028, Train_Loss: [0.3912, 0.4570, 0.4942, 0.5080, 0.4577, 0.4051, 0.4178, 0.3458]
Val_Loss: 0.635264, Val_Acc: 0.709402

Epoch #029, Train_Loss: [0.5166, 0.4380, 0.4139, 0.3783, 0.3824, 0.4719, 0.4243, 0.3587]
Val_Loss: 0.637573, Val_Acc: 0.717949

Epoch #030, Train_Loss: [0.4108, 0.3926, 0.3996, 0.4050, 0.4657, 0.3993, 0.3911, 0.5137]
Val_Loss: 0.648616, Val_Acc: 0.692308

Epoch #031, Train_Loss: [0.3816, 0.3639, 0.4192, 0.4353, 0.3614, 0.4022, 0.4514, 0.5527]
Val_Loss: 0.609311, Val_Acc: 0.726496
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #032, Train_Loss: [0.3805, 0.4175, 0.3637, 0.4477, 0.4340, 0.3044, 0.4322, 0.4915]
Val_Loss: 0.606639, Val_Acc: 0.700855
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #033, Train_Loss: [0.4575, 0.3737, 0.3442, 0.5093, 0.3410, 0.3989, 0.4090, 0.4066]
Val_Loss: 0.585484, Val_Acc: 0.726496
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #034, Train_Loss: [0.3837, 0.3839, 0.4247, 0.4372, 0.4154, 0.3371, 0.3905, 0.3509]
Val_Loss: 0.639783, Val_Acc: 0.717949

Epoch #035, Train_Loss: [0.4035, 0.4182, 0.3845, 0.3973, 0.3805, 0.3346, 0.3889, 0.5749]
Val_Loss: 0.609257, Val_Acc: 0.717949

Epoch #036, Train_Loss: [0.3361, 0.4946, 0.3860, 0.3802, 0.2995, 0.3833, 0.4713, 0.2726]
Val_Loss: 0.583108, Val_Acc: 0.760684
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #037, Train_Loss: [0.3394, 0.3968, 0.4017, 0.3409, 0.4126, 0.5098, 0.4059, 0.3208]
Val_Loss: 0.557077, Val_Acc: 0.786325
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #038, Train_Loss: [0.3934, 0.4453, 0.4346, 0.3941, 0.3809, 0.3499, 0.3132, 0.4185]
Val_Loss: 0.559408, Val_Acc: 0.735043

Epoch #039, Train_Loss: [0.3328, 0.4032, 0.4160, 0.3537, 0.3802, 0.4099, 0.3318, 0.3473]
Val_Loss: 0.575798, Val_Acc: 0.735043

Epoch #040, Train_Loss: [0.3442, 0.3761, 0.4180, 0.2982, 0.3264, 0.3897, 0.3291, 0.3604]
Val_Loss: 0.610004, Val_Acc: 0.717949

Epoch #041, Train_Loss: [0.4391, 0.3494, 0.3247, 0.3298, 0.3986, 0.4550, 0.2980, 0.4063]
Val_Loss: 0.653450, Val_Acc: 0.683761

Epoch #042, Train_Loss: [0.3296, 0.3476, 0.4019, 0.3687, 0.3761, 0.4120, 0.3839, 0.4018]
Val_Loss: 0.607128, Val_Acc: 0.700855

Epoch #043, Train_Loss: [0.3643, 0.3638, 0.3152, 0.3054, 0.3986, 0.3500, 0.3300, 0.5612]
Val_Loss: 0.606110, Val_Acc: 0.717949

Epoch #044, Train_Loss: [0.3493, 0.3968, 0.3587, 0.3190, 0.4528, 0.3343, 0.3918, 0.1918]
Val_Loss: 0.574885, Val_Acc: 0.743590

Epoch #045, Train_Loss: [0.3481, 0.2786, 0.3550, 0.3709, 0.3422, 0.3334, 0.3645, 0.3669]
Val_Loss: 0.551441, Val_Acc: 0.760684
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #046, Train_Loss: [0.3380, 0.2937, 0.3505, 0.3341, 0.3125, 0.3543, 0.3565, 0.3433]
Val_Loss: 0.555658, Val_Acc: 0.752137

Epoch #047, Train_Loss: [0.3937, 0.3834, 0.2798, 0.3001, 0.4619, 0.2733, 0.3783, 0.3114]
Val_Loss: 0.549961, Val_Acc: 0.786325
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #048, Train_Loss: [0.3934, 0.3426, 0.4297, 0.3215, 0.3379, 0.3170, 0.3360, 0.1643]
Val_Loss: 0.576000, Val_Acc: 0.752137

Epoch #049, Train_Loss: [0.3991, 0.3946, 0.3955, 0.2817, 0.2639, 0.3860, 0.3038, 0.2389]
Val_Loss: 0.535993, Val_Acc: 0.769231
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #050, Train_Loss: [0.3169, 0.3400, 0.3447, 0.2820, 0.3608, 0.4060, 0.2512, 0.2899]
Val_Loss: 0.561425, Val_Acc: 0.760684

Epoch #051, Train_Loss: [0.2263, 0.3047, 0.2759, 0.2710, 0.4493, 0.3684, 0.3786, 0.2653]
Val_Loss: 0.547139, Val_Acc: 0.769231

Epoch #052, Train_Loss: [0.2925, 0.2822, 0.2973, 0.3180, 0.3226, 0.3331, 0.3198, 0.4042]
Val_Loss: 0.583635, Val_Acc: 0.760684

Epoch #053, Train_Loss: [0.2297, 0.3476, 0.2563, 0.3171, 0.3176, 0.3184, 0.3526, 0.4441]
Val_Loss: 0.568986, Val_Acc: 0.760684

Epoch #054, Train_Loss: [0.3354, 0.3759, 0.2502, 0.2437, 0.3020, 0.3408, 0.3258, 0.2134]
Val_Loss: 0.569825, Val_Acc: 0.760684

Epoch #055, Train_Loss: [0.3878, 0.2941, 0.3366, 0.3245, 0.2861, 0.3348, 0.2789, 0.2525]
Val_Loss: 0.640604, Val_Acc: 0.717949

Epoch #056, Train_Loss: [0.3100, 0.3437, 0.2606, 0.3759, 0.2839, 0.3534, 0.3105, 0.2826]
Val_Loss: 0.673109, Val_Acc: 0.700855

Epoch #057, Train_Loss: [0.3842, 0.2874, 0.2862, 0.2695, 0.3678, 0.3589, 0.3008, 0.2563]
Val_Loss: 0.620413, Val_Acc: 0.726496

Epoch #058, Train_Loss: [0.3566, 0.3615, 0.3189, 0.2296, 0.2879, 0.2316, 0.3711, 0.2194]
Val_Loss: 0.530301, Val_Acc: 0.794872
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #059, Train_Loss: [0.2746, 0.3325, 0.3246, 0.2958, 0.3253, 0.2952, 0.2428, 0.3401]
Val_Loss: 0.557868, Val_Acc: 0.777778

Epoch #060, Train_Loss: [0.2867, 0.3002, 0.3203, 0.3548, 0.2587, 0.2667, 0.3005, 0.1882]
Val_Loss: 0.563876, Val_Acc: 0.777778

Epoch #061, Train_Loss: [0.2808, 0.3673, 0.3154, 0.2400, 0.2673, 0.1899, 0.2987, 0.4044]
Val_Loss: 0.583546, Val_Acc: 0.769231

Epoch #062, Train_Loss: [0.3455, 0.3208, 0.3162, 0.2043, 0.3332, 0.2956, 0.2239, 0.3362]
Val_Loss: 0.598432, Val_Acc: 0.743590

Epoch #063, Train_Loss: [0.2398, 0.2845, 0.3728, 0.2661, 0.2345, 0.2775, 0.2963, 0.2745]
Val_Loss: 0.607216, Val_Acc: 0.743590

Epoch #064, Train_Loss: [0.2919, 0.3271, 0.2432, 0.3069, 0.2778, 0.2523, 0.3448, 0.2569]
Val_Loss: 0.549934, Val_Acc: 0.786325

Epoch #065, Train_Loss: [0.2532, 0.2689, 0.4271, 0.2483, 0.2773, 0.2868, 0.3890, 0.1418]
Val_Loss: 0.532054, Val_Acc: 0.786325

Epoch #066, Train_Loss: [0.3074, 0.3318, 0.2856, 0.2461, 0.2497, 0.2846, 0.2902, 0.3300]
Val_Loss: 0.530426, Val_Acc: 0.777778

Epoch #067, Train_Loss: [0.3003, 0.3269, 0.3851, 0.2175, 0.2464, 0.3151, 0.2754, 0.1940]
Val_Loss: 0.546642, Val_Acc: 0.803419

Epoch #068, Train_Loss: [0.2916, 0.2606, 0.4125, 0.2865, 0.2978, 0.2623, 0.3030, 0.1825]
Val_Loss: 0.620977, Val_Acc: 0.743590

Epoch #069, Train_Loss: [0.3571, 0.2425, 0.3161, 0.3063, 0.2410, 0.2022, 0.3072, 0.2415]
Val_Loss: 0.642457, Val_Acc: 0.743590

Epoch #070, Train_Loss: [0.2509, 0.3611, 0.2320, 0.1847, 0.3249, 0.2809, 0.2373, 0.2772]
Val_Loss: 0.661833, Val_Acc: 0.743590

Epoch #071, Train_Loss: [0.2062, 0.3102, 0.3062, 0.3460, 0.2170, 0.3497, 0.2259, 0.3514]
Val_Loss: 0.535934, Val_Acc: 0.803419

Epoch #072, Train_Loss: [0.2067, 0.2343, 0.2956, 0.2841, 0.3284, 0.3294, 0.2559, 0.4182]
Val_Loss: 0.536377, Val_Acc: 0.794872

Epoch #073, Train_Loss: [0.2551, 0.2425, 0.2908, 0.1974, 0.3486, 0.2247, 0.3608, 0.2083]
Val_Loss: 0.530045, Val_Acc: 0.777778
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #074, Train_Loss: [0.2052, 0.3096, 0.2188, 0.3370, 0.3340, 0.2331, 0.2414, 0.1801]
Val_Loss: 0.534168, Val_Acc: 0.769231

Epoch #075, Train_Loss: [0.3197, 0.1843, 0.2579, 0.2558, 0.2225, 0.2214, 0.2537, 0.5241]
Val_Loss: 0.560651, Val_Acc: 0.811966

Epoch #076, Train_Loss: [0.2149, 0.2932, 0.3447, 0.2185, 0.2636, 0.2068, 0.2257, 0.3150]
Val_Loss: 0.644331, Val_Acc: 0.752137

Epoch #077, Train_Loss: [0.2417, 0.2417, 0.1961, 0.2790, 0.2576, 0.2894, 0.2907, 0.1341]
Val_Loss: 0.653247, Val_Acc: 0.743590

Epoch #078, Train_Loss: [0.2809, 0.2206, 0.2502, 0.2722, 0.2600, 0.3147, 0.2371, 0.1932]
Val_Loss: 0.605748, Val_Acc: 0.760684

Epoch #079, Train_Loss: [0.2142, 0.2442, 0.2083, 0.2862, 0.2714, 0.2186, 0.2933, 0.1378]
Val_Loss: 0.606191, Val_Acc: 0.777778

Epoch #080, Train_Loss: [0.2367, 0.1852, 0.3366, 0.2870, 0.1686, 0.2731, 0.2614, 0.1928]
Val_Loss: 0.569157, Val_Acc: 0.786325

Epoch #081, Train_Loss: [0.2388, 0.2349, 0.2000, 0.1844, 0.2285, 0.2542, 0.2171, 0.2457]
Val_Loss: 0.573644, Val_Acc: 0.803419

Epoch #082, Train_Loss: [0.1651, 0.2554, 0.2869, 0.2118, 0.1450, 0.2290, 0.2215, 0.2848]
Val_Loss: 0.580945, Val_Acc: 0.803419

Epoch #083, Train_Loss: [0.2191, 0.1755, 0.2864, 0.2317, 0.1780, 0.1821, 0.1696, 0.1785]
Val_Loss: 0.580284, Val_Acc: 0.794872

Epoch #084, Train_Loss: [0.1154, 0.2473, 0.2790, 0.2079, 0.2346, 0.2655, 0.1927, 0.3265]
Val_Loss: 0.582477, Val_Acc: 0.786325

Epoch #085, Train_Loss: [0.2100, 0.2068, 0.1517, 0.2024, 0.1919, 0.2774, 0.2250, 0.2703]
Val_Loss: 0.579016, Val_Acc: 0.803419

Epoch #086, Train_Loss: [0.1904, 0.1543, 0.1639, 0.1588, 0.2583, 0.3058, 0.1733, 0.1587]
Val_Loss: 0.617589, Val_Acc: 0.777778

Epoch #087, Train_Loss: [0.1494, 0.2300, 0.1295, 0.2850, 0.2078, 0.2325, 0.1773, 0.1499]
Val_Loss: 0.597298, Val_Acc: 0.803419

Epoch #088, Train_Loss: [0.2267, 0.1644, 0.1914, 0.1501, 0.1821, 0.1537, 0.2363, 0.2540]
Val_Loss: 0.613939, Val_Acc: 0.786325

Epoch #089, Train_Loss: [0.2164, 0.2327, 0.2001, 0.2568, 0.1437, 0.1953, 0.1320, 0.1237]
Val_Loss: 0.589421, Val_Acc: 0.811966

Epoch #090, Train_Loss: [0.2077, 0.0989, 0.1114, 0.1693, 0.3062, 0.1866, 0.2229, 0.2336]
Val_Loss: 0.590586, Val_Acc: 0.786325

Epoch #091, Train_Loss: [0.1300, 0.2022, 0.2134, 0.1619, 0.1615, 0.2796, 0.1704, 0.1469]
Val_Loss: 0.588577, Val_Acc: 0.811966

Epoch #092, Train_Loss: [0.2110, 0.1944, 0.1310, 0.1658, 0.1866, 0.1665, 0.2291, 0.1048]
Val_Loss: 0.579402, Val_Acc: 0.803419

Epoch #093, Train_Loss: [0.1580, 0.2413, 0.1586, 0.1878, 0.1210, 0.1588, 0.1557, 0.2131]
Val_Loss: 0.657016, Val_Acc: 0.777778

Epoch #094, Train_Loss: [0.2636, 0.1301, 0.1853, 0.1814, 0.1795, 0.2531, 0.1377, 0.1906]
Val_Loss: 0.671309, Val_Acc: 0.769231

Epoch #095, Train_Loss: [0.2222, 0.1955, 0.2239, 0.1380, 0.1868, 0.1912, 0.1885, 0.0792]
Val_Loss: 0.649722, Val_Acc: 0.760684

Epoch #096, Train_Loss: [0.1500, 0.2160, 0.1451, 0.1896, 0.1816, 0.1235, 0.1757, 0.1263]
Val_Loss: 0.616559, Val_Acc: 0.803419

Epoch #097, Train_Loss: [0.1480, 0.2483, 0.2456, 0.1213, 0.1443, 0.1657, 0.1978, 0.2200]
Val_Loss: 0.693443, Val_Acc: 0.777778

Epoch #098, Train_Loss: [0.1811, 0.2018, 0.1636, 0.1828, 0.1606, 0.1513, 0.1296, 0.1488]
Val_Loss: 0.582159, Val_Acc: 0.829060

Epoch #099, Train_Loss: [0.1654, 0.1531, 0.0663, 0.1331, 0.1946, 0.1611, 0.2604, 0.1720]
Val_Loss: 0.600939, Val_Acc: 0.803419

Epoch #100, Train_Loss: [0.1607, 0.1129, 0.1957, 0.2402, 0.1576, 0.1229, 0.1685, 0.1745]
Val_Loss: 0.649822, Val_Acc: 0.786325

Epoch #101, Train_Loss: [0.1921, 0.1386, 0.1657, 0.1712, 0.1932, 0.1264, 0.1505, 0.1374]
Val_Loss: 0.682410, Val_Acc: 0.769231

Epoch #102, Train_Loss: [0.2026, 0.1343, 0.1315, 0.1831, 0.1504, 0.1197, 0.2357, 0.1419]
Val_Loss: 0.599430, Val_Acc: 0.829060

Epoch #103, Train_Loss: [0.2036, 0.1371, 0.1422, 0.0918, 0.1505, 0.2351, 0.1375, 0.0780]
Val_Loss: 0.608127, Val_Acc: 0.820513

Epoch #104, Train_Loss: [0.1663, 0.1987, 0.1465, 0.1015, 0.1452, 0.1808, 0.1391, 0.0602]
Val_Loss: 0.626462, Val_Acc: 0.811966

Epoch #105, Train_Loss: [0.1632, 0.2184, 0.1179, 0.1728, 0.1413, 0.0750, 0.1383, 0.1534]
Val_Loss: 0.675367, Val_Acc: 0.769231

Epoch #106, Train_Loss: [0.1563, 0.1847, 0.1414, 0.1013, 0.1326, 0.2055, 0.1713, 0.0987]
Val_Loss: 0.719033, Val_Acc: 0.786325

Epoch #107, Train_Loss: [0.2530, 0.1400, 0.1274, 0.1467, 0.1045, 0.1270, 0.1568, 0.0527]
Val_Loss: 0.650827, Val_Acc: 0.777778

Epoch #108, Train_Loss: [0.0995, 0.1350, 0.1409, 0.1449, 0.1620, 0.1011, 0.1209, 0.1931]
Val_Loss: 0.680662, Val_Acc: 0.786325

Epoch #109, Train_Loss: [0.0954, 0.1845, 0.1525, 0.1397, 0.0966, 0.1214, 0.1501, 0.1348]
Val_Loss: 0.624320, Val_Acc: 0.820513

Epoch #110, Train_Loss: [0.1624, 0.0993, 0.1099, 0.1338, 0.0835, 0.1304, 0.1109, 0.1684]
Val_Loss: 0.647647, Val_Acc: 0.811966

Epoch #111, Train_Loss: [0.1248, 0.2221, 0.1612, 0.1403, 0.2087, 0.0985, 0.0880, 0.1937]
Val_Loss: 0.642940, Val_Acc: 0.811966

Epoch #112, Train_Loss: [0.0890, 0.0960, 0.0671, 0.1537, 0.1763, 0.1436, 0.1280, 0.1952]
Val_Loss: 0.637143, Val_Acc: 0.811966

Epoch #113, Train_Loss: [0.1989, 0.1080, 0.1516, 0.1257, 0.0998, 0.1179, 0.1250, 0.0962]
Val_Loss: 0.637409, Val_Acc: 0.820513

Epoch #114, Train_Loss: [0.0993, 0.1279, 0.0745, 0.0955, 0.1067, 0.1619, 0.1529, 0.2332]
Val_Loss: 0.668069, Val_Acc: 0.811966

********** TEST START **********
Reload Best Model
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********
TEST :: Test_Acc: 0.613445
