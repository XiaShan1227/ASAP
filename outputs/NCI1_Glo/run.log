+-----------------+-------------------+
|    Parameter    |       Value       |
+=================+===================+
| Batch size      | 128               |
+-----------------+-------------------+
| Dataset         | NCI1              |
+-----------------+-------------------+
| Dropout ratio   | 0.5               |
+-----------------+-------------------+
| Epochs          | 10000             |
+-----------------+-------------------+
| Exp name        | NCI1_Glo          |
+-----------------+-------------------+
| Gpu index       | 0                 |
+-----------------+-------------------+
| Hid             | 128               |
+-----------------+-------------------+
| Lr              | 0.0005            |
+-----------------+-------------------+
| Model           | ASAPooling_Global |
+-----------------+-------------------+
| Patience        | 40                |
+-----------------+-------------------+
| Pooling ratio   | 0.5               |
+-----------------+-------------------+
| Seed            | 16                |
+-----------------+-------------------+
| Test batch size | 1                 |
+-----------------+-------------------+
| Weight decay    | 0.0001            |
+-----------------+-------------------+
Using GPU: 0
ASAPooling_Global(
  (conv1): GCNConv(37, 128)
  (conv2): GCNConv(128, 128)
  (conv3): GCNConv(128, 128)
  (pool): ASAPooling(384, ratio=0.5)
  (lin1): Linear(in_features=768, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Model Parameter: 442310
Using Adam

Epoch #000, Train_Loss: [0.6866, 0.6895, 0.7069, 0.7050, 0.6941, 0.6919, 0.6952, 0.6882, 0.6933, 0.6947, 0.6959, 0.6869, 0.6932, 0.6990, 0.6905, 0.6934, 0.6922, 0.7002, 0.6951, 0.6950, 0.6957, 0.6926, 0.6927, 0.6923, 0.6940, 0.6930]
Val_Loss: 0.693001, Val_Acc: 0.510949
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #001, Train_Loss: [0.6935, 0.6939, 0.6924, 0.6950, 0.6925, 0.6923, 0.6931, 0.6937, 0.6936, 0.6915, 0.6925, 0.6915, 0.6925, 0.6930, 0.6916, 0.6927, 0.6936, 0.6933, 0.6901, 0.6942, 0.6949, 0.6937, 0.6908, 0.6940, 0.6932, 0.6937]
Val_Loss: 0.692491, Val_Acc: 0.530414
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #002, Train_Loss: [0.6937, 0.6916, 0.6927, 0.6903, 0.6918, 0.6902, 0.6925, 0.6924, 0.6883, 0.6901, 0.6924, 0.6940, 0.6900, 0.6922, 0.6903, 0.6873, 0.6935, 0.6927, 0.6957, 0.6872, 0.6928, 0.6896, 0.6851, 0.6903, 0.6916, 0.6885]
Val_Loss: 0.689549, Val_Acc: 0.562044
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #003, Train_Loss: [0.6879, 0.6895, 0.6875, 0.6907, 0.6811, 0.6868, 0.6857, 0.6803, 0.6874, 0.6799, 0.6822, 0.6829, 0.6862, 0.6835, 0.6809, 0.6753, 0.6943, 0.6846, 0.7048, 0.6813, 0.6721, 0.6678, 0.6934, 0.6781, 0.6722, 0.6868]
Val_Loss: 0.683988, Val_Acc: 0.545012
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #004, Train_Loss: [0.6845, 0.6726, 0.6865, 0.7038, 0.7025, 0.6552, 0.6716, 0.6583, 0.6722, 0.7007, 0.6809, 0.6604, 0.6618, 0.6698, 0.6617, 0.6701, 0.6405, 0.6531, 0.6713, 0.7130, 0.6870, 0.6766, 0.6595, 0.7144, 0.6627, 0.6751]
Val_Loss: 0.682657, Val_Acc: 0.576642
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #005, Train_Loss: [0.6899, 0.6904, 0.6333, 0.6673, 0.6476, 0.6278, 0.6694, 0.7147, 0.6655, 0.6626, 0.6645, 0.6755, 0.6625, 0.6642, 0.7069, 0.6608, 0.6393, 0.6886, 0.6580, 0.6703, 0.6774, 0.6466, 0.7098, 0.6826, 0.6565, 0.6728]
Val_Loss: 0.676751, Val_Acc: 0.593674
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #006, Train_Loss: [0.6875, 0.6981, 0.6731, 0.6449, 0.6513, 0.6882, 0.6657, 0.6858, 0.6496, 0.6626, 0.6631, 0.6248, 0.7093, 0.6785, 0.6843, 0.6388, 0.6515, 0.6605, 0.6483, 0.6826, 0.6877, 0.6734, 0.6939, 0.6317, 0.6581, 0.6571]
Val_Loss: 0.673564, Val_Acc: 0.598540
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #007, Train_Loss: [0.6492, 0.6531, 0.6535, 0.6511, 0.6751, 0.6550, 0.6714, 0.6745, 0.6442, 0.6905, 0.6468, 0.6591, 0.6887, 0.6913, 0.6430, 0.6546, 0.6342, 0.6598, 0.6463, 0.6291, 0.6518, 0.6313, 0.6299, 0.6656, 0.6543, 0.5992]
Val_Loss: 0.660400, Val_Acc: 0.610706
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #008, Train_Loss: [0.6424, 0.6857, 0.6309, 0.6395, 0.6215, 0.6337, 0.6717, 0.6810, 0.6531, 0.6235, 0.6091, 0.5866, 0.6991, 0.6761, 0.6071, 0.6107, 0.6499, 0.6344, 0.6394, 0.6805, 0.6540, 0.6522, 0.7015, 0.6474, 0.6450, 0.6552]
Val_Loss: 0.648856, Val_Acc: 0.613139
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #009, Train_Loss: [0.6834, 0.6099, 0.6136, 0.6487, 0.6248, 0.6480, 0.6147, 0.6480, 0.6481, 0.6747, 0.6378, 0.6309, 0.6590, 0.6406, 0.5967, 0.6245, 0.6519, 0.6542, 0.6791, 0.6134, 0.6605, 0.6237, 0.6384, 0.6283, 0.6338, 0.6325]
Val_Loss: 0.644280, Val_Acc: 0.627737
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #010, Train_Loss: [0.6622, 0.6235, 0.6063, 0.6058, 0.6956, 0.6472, 0.6227, 0.6129, 0.6144, 0.5826, 0.6032, 0.6052, 0.6388, 0.6774, 0.6606, 0.6175, 0.6323, 0.6607, 0.6153, 0.6130, 0.6571, 0.6285, 0.6221, 0.6690, 0.6130, 0.6084]
Val_Loss: 0.632249, Val_Acc: 0.642336
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #011, Train_Loss: [0.6157, 0.6230, 0.6446, 0.5881, 0.6233, 0.6879, 0.6251, 0.5984, 0.6357, 0.5975, 0.6630, 0.6363, 0.6641, 0.6293, 0.6031, 0.6119, 0.6638, 0.6445, 0.6189, 0.6504, 0.5808, 0.6455, 0.6120, 0.6016, 0.6145, 0.6161]
Val_Loss: 0.626901, Val_Acc: 0.654501
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #012, Train_Loss: [0.5773, 0.6007, 0.6038, 0.6561, 0.6000, 0.6321, 0.5522, 0.6442, 0.5839, 0.6675, 0.6262, 0.6110, 0.6318, 0.6449, 0.6356, 0.6236, 0.5827, 0.6319, 0.6516, 0.5997, 0.6217, 0.6521, 0.6383, 0.6106, 0.5725, 0.6136]
Val_Loss: 0.626993, Val_Acc: 0.664234

Epoch #013, Train_Loss: [0.6111, 0.6472, 0.5760, 0.6172, 0.6506, 0.6226, 0.6203, 0.6316, 0.5933, 0.5986, 0.6203, 0.6429, 0.5509, 0.5851, 0.6182, 0.6090, 0.6610, 0.6310, 0.6503, 0.6487, 0.6678, 0.6282, 0.5924, 0.5952, 0.6113, 0.5723]
Val_Loss: 0.631633, Val_Acc: 0.635036

Epoch #014, Train_Loss: [0.6756, 0.6048, 0.6207, 0.6328, 0.5833, 0.5910, 0.5624, 0.6164, 0.6146, 0.5824, 0.6066, 0.5719, 0.6295, 0.6566, 0.5754, 0.6504, 0.6403, 0.6788, 0.6200, 0.6254, 0.6236, 0.5848, 0.6072, 0.6166, 0.6074, 0.6323]
Val_Loss: 0.623332, Val_Acc: 0.649635
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #015, Train_Loss: [0.6041, 0.5391, 0.6042, 0.6482, 0.5781, 0.6088, 0.6227, 0.6609, 0.6422, 0.5908, 0.6397, 0.6059, 0.5704, 0.6401, 0.6190, 0.6365, 0.6005, 0.6000, 0.6055, 0.5886, 0.6556, 0.5981, 0.6457, 0.6733, 0.6327, 0.5714]
Val_Loss: 0.629416, Val_Acc: 0.654501

Epoch #016, Train_Loss: [0.6249, 0.6058, 0.6021, 0.5771, 0.5767, 0.5833, 0.6247, 0.5906, 0.6075, 0.5998, 0.6154, 0.5533, 0.5809, 0.6069, 0.5928, 0.6572, 0.6561, 0.5767, 0.5896, 0.5951, 0.5944, 0.6056, 0.6391, 0.6413, 0.6223, 0.6379]
Val_Loss: 0.619659, Val_Acc: 0.661800
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #017, Train_Loss: [0.5982, 0.5867, 0.5848, 0.6254, 0.5764, 0.6042, 0.6183, 0.6085, 0.5786, 0.6096, 0.6069, 0.5981, 0.6000, 0.5833, 0.6047, 0.6035, 0.5472, 0.5971, 0.6405, 0.6047, 0.5333, 0.5836, 0.5805, 0.6103, 0.5861, 0.5900]
Val_Loss: 0.634028, Val_Acc: 0.649635

Epoch #018, Train_Loss: [0.5770, 0.5551, 0.5554, 0.5968, 0.6329, 0.6787, 0.5460, 0.5978, 0.6410, 0.5526, 0.6039, 0.5543, 0.5817, 0.6983, 0.5572, 0.5764, 0.6265, 0.6228, 0.5953, 0.6096, 0.5962, 0.5367, 0.6051, 0.6305, 0.5905, 0.6460]
Val_Loss: 0.625343, Val_Acc: 0.666667

Epoch #019, Train_Loss: [0.5464, 0.5947, 0.5778, 0.5584, 0.6215, 0.6017, 0.6038, 0.5853, 0.5858, 0.6682, 0.5166, 0.5395, 0.6311, 0.5488, 0.5812, 0.5823, 0.5661, 0.6016, 0.6037, 0.5933, 0.5979, 0.5985, 0.6369, 0.5402, 0.6628, 0.6366]
Val_Loss: 0.636719, Val_Acc: 0.669100

Epoch #020, Train_Loss: [0.5803, 0.5629, 0.5991, 0.5502, 0.6521, 0.6413, 0.6054, 0.6106, 0.6178, 0.6227, 0.5605, 0.6367, 0.5593, 0.5448, 0.6397, 0.6106, 0.6652, 0.5935, 0.5287, 0.5845, 0.5546, 0.4993, 0.6740, 0.5790, 0.5930, 0.5690]
Val_Loss: 0.643957, Val_Acc: 0.639903

Epoch #021, Train_Loss: [0.6249, 0.6284, 0.5784, 0.5573, 0.6081, 0.6175, 0.5597, 0.5703, 0.5624, 0.6113, 0.5222, 0.5643, 0.5926, 0.5514, 0.5642, 0.5742, 0.6358, 0.6135, 0.5672, 0.5830, 0.5756, 0.6277, 0.5809, 0.5847, 0.5803, 0.5048]
Val_Loss: 0.623877, Val_Acc: 0.686131

Epoch #022, Train_Loss: [0.5868, 0.5719, 0.5351, 0.5649, 0.5731, 0.5549, 0.6356, 0.5450, 0.5359, 0.5586, 0.5341, 0.6010, 0.6592, 0.6121, 0.5456, 0.6057, 0.5802, 0.5567, 0.6015, 0.6097, 0.5005, 0.5611, 0.6181, 0.5783, 0.5885, 0.5582]
Val_Loss: 0.624717, Val_Acc: 0.683698

Epoch #023, Train_Loss: [0.5894, 0.5386, 0.5681, 0.5388, 0.5618, 0.5164, 0.6091, 0.6275, 0.5878, 0.6201, 0.5585, 0.5570, 0.5376, 0.6061, 0.6807, 0.6008, 0.5484, 0.5058, 0.5937, 0.5348, 0.5216, 0.6034, 0.5701, 0.5992, 0.5529, 0.5867]
Val_Loss: 0.625059, Val_Acc: 0.686131

Epoch #024, Train_Loss: [0.6180, 0.5648, 0.5984, 0.5407, 0.5798, 0.5382, 0.5591, 0.6206, 0.5551, 0.5468, 0.5791, 0.6140, 0.5755, 0.5281, 0.5777, 0.6174, 0.6087, 0.5686, 0.6111, 0.5728, 0.5661, 0.5714, 0.5851, 0.5885, 0.6253, 0.5083]
Val_Loss: 0.624778, Val_Acc: 0.690998

Epoch #025, Train_Loss: [0.6267, 0.6078, 0.5915, 0.5678, 0.5952, 0.6001, 0.5713, 0.5217, 0.5903, 0.5425, 0.5475, 0.5640, 0.6285, 0.5699, 0.6138, 0.4995, 0.5456, 0.5852, 0.5777, 0.5445, 0.5456, 0.5557, 0.5668, 0.6319, 0.5448, 0.6081]
Val_Loss: 0.646018, Val_Acc: 0.664234

Epoch #026, Train_Loss: [0.5645, 0.5865, 0.5994, 0.5451, 0.6339, 0.5641, 0.4904, 0.5672, 0.5921, 0.5607, 0.5594, 0.5879, 0.5392, 0.5162, 0.5880, 0.5539, 0.5634, 0.5770, 0.5936, 0.5635, 0.5350, 0.5981, 0.6091, 0.5178, 0.5534, 0.6024]
Val_Loss: 0.634151, Val_Acc: 0.686131

Epoch #027, Train_Loss: [0.5487, 0.5912, 0.5763, 0.5499, 0.5139, 0.5450, 0.6276, 0.5471, 0.5584, 0.6535, 0.5471, 0.5995, 0.6306, 0.6046, 0.5278, 0.5519, 0.6090, 0.5036, 0.5810, 0.5656, 0.5296, 0.5216, 0.6101, 0.6128, 0.6171, 0.5818]
Val_Loss: 0.632219, Val_Acc: 0.705596

Epoch #028, Train_Loss: [0.5404, 0.5279, 0.5598, 0.5919, 0.5288, 0.5858, 0.6057, 0.5989, 0.5817, 0.5343, 0.5848, 0.5612, 0.6048, 0.5706, 0.6338, 0.5338, 0.6330, 0.5502, 0.5611, 0.5684, 0.5519, 0.6187, 0.5888, 0.5576, 0.6311, 0.5999]
Val_Loss: 0.645370, Val_Acc: 0.678832

Epoch #029, Train_Loss: [0.5507, 0.6149, 0.5891, 0.5476, 0.5790, 0.5991, 0.5127, 0.5716, 0.5404, 0.5770, 0.5122, 0.5105, 0.5389, 0.5615, 0.5503, 0.5983, 0.5870, 0.5921, 0.5574, 0.5893, 0.5945, 0.5467, 0.5671, 0.6769, 0.5471, 0.5076]
Val_Loss: 0.642622, Val_Acc: 0.683698

Epoch #030, Train_Loss: [0.4981, 0.5937, 0.5625, 0.5923, 0.5076, 0.5427, 0.5996, 0.5471, 0.5196, 0.5131, 0.5530, 0.5363, 0.5678, 0.5727, 0.6102, 0.6604, 0.5768, 0.5395, 0.5267, 0.6015, 0.4857, 0.5831, 0.4902, 0.6384, 0.5396, 0.5937]
Val_Loss: 0.693186, Val_Acc: 0.635036

Epoch #031, Train_Loss: [0.6346, 0.5806, 0.5511, 0.6454, 0.5935, 0.5419, 0.6005, 0.6123, 0.5952, 0.6220, 0.5764, 0.5760, 0.5882, 0.6149, 0.6126, 0.6436, 0.5746, 0.5773, 0.5535, 0.5958, 0.5724, 0.6038, 0.5662, 0.5766, 0.6122, 0.5811]
Val_Loss: 0.626614, Val_Acc: 0.703163

Epoch #032, Train_Loss: [0.5419, 0.5161, 0.5313, 0.6192, 0.6610, 0.5139, 0.5499, 0.6234, 0.6097, 0.5603, 0.5616, 0.4683, 0.5563, 0.5262, 0.5831, 0.5862, 0.4757, 0.6127, 0.5254, 0.5938, 0.5752, 0.5291, 0.5089, 0.6062, 0.5656, 0.6167]
Val_Loss: 0.640699, Val_Acc: 0.688564

Epoch #033, Train_Loss: [0.5555, 0.5383, 0.5370, 0.5565, 0.5517, 0.5227, 0.6101, 0.5560, 0.5685, 0.5629, 0.5304, 0.5528, 0.5231, 0.5034, 0.5583, 0.5215, 0.5658, 0.5027, 0.4729, 0.5710, 0.5677, 0.5664, 0.5446, 0.5174, 0.5543, 0.6271]
Val_Loss: 0.640029, Val_Acc: 0.705596

Epoch #034, Train_Loss: [0.5225, 0.5399, 0.5952, 0.5294, 0.5127, 0.5564, 0.5590, 0.5562, 0.5597, 0.5939, 0.6026, 0.5997, 0.5624, 0.5928, 0.5284, 0.5395, 0.5476, 0.5771, 0.5228, 0.5357, 0.5717, 0.5697, 0.5443, 0.5141, 0.6059, 0.4952]
Val_Loss: 0.637606, Val_Acc: 0.688564

Epoch #035, Train_Loss: [0.5293, 0.5183, 0.6161, 0.5774, 0.5154, 0.5623, 0.5889, 0.5910, 0.6494, 0.5078, 0.6036, 0.5668, 0.5930, 0.6353, 0.5627, 0.5253, 0.4906, 0.5235, 0.5822, 0.4952, 0.5789, 0.6264, 0.5498, 0.4697, 0.5448, 0.5692]
Val_Loss: 0.631303, Val_Acc: 0.690998

Epoch #036, Train_Loss: [0.5554, 0.5016, 0.5301, 0.4688, 0.6326, 0.5969, 0.6150, 0.6004, 0.5182, 0.4796, 0.5487, 0.5104, 0.5244, 0.5221, 0.5516, 0.5632, 0.5958, 0.5097, 0.5595, 0.5346, 0.5500, 0.4923, 0.5806, 0.6015, 0.5202, 0.6352]
Val_Loss: 0.640368, Val_Acc: 0.690998

Epoch #037, Train_Loss: [0.5228, 0.5376, 0.5357, 0.5990, 0.5993, 0.6641, 0.5671, 0.5510, 0.6055, 0.4796, 0.5842, 0.5428, 0.4623, 0.5619, 0.5320, 0.6275, 0.5382, 0.5327, 0.5464, 0.5551, 0.4864, 0.5872, 0.4934, 0.5141, 0.5199, 0.5683]
Val_Loss: 0.645142, Val_Acc: 0.695864

Epoch #038, Train_Loss: [0.5439, 0.5375, 0.6348, 0.5167, 0.5338, 0.5240, 0.5232, 0.5275, 0.5473, 0.5195, 0.4756, 0.5425, 0.5473, 0.4830, 0.4815, 0.5510, 0.6328, 0.5176, 0.5934, 0.5363, 0.5120, 0.5567, 0.5595, 0.5618, 0.5671, 0.5279]
Val_Loss: 0.637191, Val_Acc: 0.688564

Epoch #039, Train_Loss: [0.5775, 0.5627, 0.5642, 0.5435, 0.5632, 0.5857, 0.5187, 0.5882, 0.5020, 0.5142, 0.5359, 0.5852, 0.5430, 0.5528, 0.5429, 0.5290, 0.5653, 0.4975, 0.4909, 0.4871, 0.5727, 0.5229, 0.5175, 0.4950, 0.5436, 0.4992]
Val_Loss: 0.651159, Val_Acc: 0.695864

Epoch #040, Train_Loss: [0.6142, 0.5161, 0.5338, 0.5506, 0.5685, 0.5336, 0.5162, 0.4934, 0.4870, 0.5704, 0.6016, 0.5011, 0.5057, 0.5386, 0.5382, 0.5009, 0.5337, 0.6258, 0.5311, 0.5044, 0.5316, 0.6117, 0.6135, 0.5295, 0.5692, 0.5338]
Val_Loss: 0.640359, Val_Acc: 0.690998

Epoch #041, Train_Loss: [0.5234, 0.5472, 0.5759, 0.4772, 0.5324, 0.5186, 0.5190, 0.5517, 0.5617, 0.5723, 0.4847, 0.5444, 0.5217, 0.5388, 0.5394, 0.5442, 0.5488, 0.5704, 0.6002, 0.5365, 0.5347, 0.4933, 0.5954, 0.5395, 0.5076, 0.5634]
Val_Loss: 0.639786, Val_Acc: 0.695864

Epoch #042, Train_Loss: [0.5145, 0.5989, 0.5282, 0.5229, 0.5072, 0.5148, 0.5778, 0.5434, 0.5556, 0.6070, 0.5349, 0.4487, 0.5215, 0.5398, 0.5662, 0.5276, 0.5745, 0.5473, 0.5362, 0.5282, 0.5327, 0.5313, 0.4810, 0.4731, 0.4950, 0.4922]
Val_Loss: 0.640361, Val_Acc: 0.698297

Epoch #043, Train_Loss: [0.5189, 0.5439, 0.5121, 0.4494, 0.5062, 0.5081, 0.5181, 0.5161, 0.5326, 0.5177, 0.6499, 0.6409, 0.4969, 0.5387, 0.5422, 0.4840, 0.5120, 0.5148, 0.4877, 0.5546, 0.5865, 0.5139, 0.5877, 0.5416, 0.5548, 0.5631]
Val_Loss: 0.646077, Val_Acc: 0.698297

Epoch #044, Train_Loss: [0.5506, 0.5562, 0.5492, 0.5673, 0.5159, 0.5254, 0.4941, 0.5466, 0.5431, 0.5822, 0.4877, 0.5172, 0.5240, 0.4526, 0.4917, 0.5474, 0.5599, 0.5796, 0.5481, 0.4912, 0.4624, 0.6343, 0.5010, 0.4780, 0.4442, 0.6484]
Val_Loss: 0.646832, Val_Acc: 0.688564

Epoch #045, Train_Loss: [0.6096, 0.4752, 0.4227, 0.5823, 0.5831, 0.5435, 0.4651, 0.5566, 0.5554, 0.4941, 0.5382, 0.5361, 0.4699, 0.5147, 0.5733, 0.4895, 0.4969, 0.5617, 0.5182, 0.5690, 0.5871, 0.4908, 0.5451, 0.4983, 0.5311, 0.4825]
Val_Loss: 0.641328, Val_Acc: 0.698297

Epoch #046, Train_Loss: [0.5699, 0.4961, 0.5407, 0.5585, 0.4973, 0.5953, 0.5243, 0.5399, 0.5283, 0.5601, 0.6509, 0.5358, 0.4741, 0.5756, 0.4954, 0.5670, 0.4718, 0.5795, 0.5468, 0.4874, 0.5473, 0.5007, 0.4571, 0.4409, 0.5323, 0.4378]
Val_Loss: 0.647166, Val_Acc: 0.708029

Epoch #047, Train_Loss: [0.5366, 0.5412, 0.4540, 0.5161, 0.4930, 0.5184, 0.5953, 0.5333, 0.5273, 0.5825, 0.4773, 0.5101, 0.5583, 0.5322, 0.5434, 0.4513, 0.5168, 0.5338, 0.4956, 0.5685, 0.4872, 0.4923, 0.4862, 0.5561, 0.6075, 0.5619]
Val_Loss: 0.633874, Val_Acc: 0.698297

Epoch #048, Train_Loss: [0.4816, 0.5094, 0.6042, 0.5116, 0.4714, 0.5242, 0.4600, 0.5651, 0.4960, 0.5050, 0.5745, 0.4774, 0.5362, 0.4848, 0.5098, 0.6070, 0.5124, 0.4992, 0.5364, 0.5067, 0.5063, 0.4764, 0.5278, 0.6600, 0.5095, 0.5250]
Val_Loss: 0.634007, Val_Acc: 0.703163

Epoch #049, Train_Loss: [0.5840, 0.4890, 0.5314, 0.4848, 0.4601, 0.5318, 0.5630, 0.5316, 0.5292, 0.5729, 0.5716, 0.5197, 0.5041, 0.5936, 0.5360, 0.5335, 0.5785, 0.5622, 0.5616, 0.4838, 0.4702, 0.5343, 0.4916, 0.4862, 0.4901, 0.4271]
Val_Loss: 0.633526, Val_Acc: 0.705596

Epoch #050, Train_Loss: [0.6315, 0.5708, 0.5892, 0.5453, 0.5238, 0.5069, 0.5327, 0.5095, 0.5243, 0.4599, 0.4603, 0.5424, 0.5540, 0.4340, 0.5041, 0.5300, 0.4700, 0.4774, 0.5442, 0.6059, 0.5820, 0.5703, 0.4581, 0.4748, 0.4510, 0.5333]
Val_Loss: 0.652521, Val_Acc: 0.695864

Epoch #051, Train_Loss: [0.6073, 0.5029, 0.5159, 0.5347, 0.5068, 0.4297, 0.5560, 0.6064, 0.5034, 0.5152, 0.4402, 0.5084, 0.5375, 0.4778, 0.4964, 0.4887, 0.5472, 0.5164, 0.5314, 0.5008, 0.4077, 0.5251, 0.5671, 0.5353, 0.5071, 0.5752]
Val_Loss: 0.629026, Val_Acc: 0.712895

Epoch #052, Train_Loss: [0.5157, 0.4925, 0.4839, 0.4312, 0.5401, 0.5063, 0.5256, 0.5535, 0.5654, 0.5487, 0.4859, 0.5662, 0.5110, 0.5108, 0.5645, 0.5723, 0.4492, 0.5353, 0.5912, 0.5001, 0.5290, 0.4398, 0.5092, 0.5576, 0.5125, 0.4397]
Val_Loss: 0.646645, Val_Acc: 0.698297

Epoch #053, Train_Loss: [0.5318, 0.4751, 0.5592, 0.5225, 0.4995, 0.4599, 0.5606, 0.4915, 0.4288, 0.5160, 0.5011, 0.5299, 0.5800, 0.5024, 0.5438, 0.5806, 0.4166, 0.5411, 0.5904, 0.5373, 0.5017, 0.5268, 0.5182, 0.5238, 0.6177, 0.5089]
Val_Loss: 0.628824, Val_Acc: 0.705596

Epoch #054, Train_Loss: [0.4795, 0.5319, 0.5383, 0.4168, 0.5024, 0.4905, 0.5127, 0.5355, 0.5015, 0.4588, 0.5220, 0.4999, 0.5288, 0.4995, 0.5459, 0.4371, 0.5204, 0.6167, 0.5375, 0.5121, 0.4863, 0.5655, 0.5656, 0.5251, 0.5035, 0.5185]
Val_Loss: 0.637666, Val_Acc: 0.712895

Epoch #055, Train_Loss: [0.4858, 0.5319, 0.5345, 0.4700, 0.5313, 0.5789, 0.5631, 0.4998, 0.5856, 0.4434, 0.5026, 0.5052, 0.4527, 0.4635, 0.5863, 0.5089, 0.5560, 0.4410, 0.5248, 0.5028, 0.5723, 0.5011, 0.4659, 0.4974, 0.5082, 0.6073]
Val_Loss: 0.633050, Val_Acc: 0.717762

Epoch #056, Train_Loss: [0.5476, 0.5552, 0.5828, 0.5040, 0.4786, 0.5162, 0.5181, 0.4570, 0.5744, 0.5331, 0.5139, 0.5359, 0.5000, 0.5183, 0.4916, 0.4581, 0.5710, 0.4897, 0.5047, 0.4962, 0.4719, 0.5195, 0.5026, 0.5500, 0.4903, 0.4332]
Val_Loss: 0.678641, Val_Acc: 0.695864

Epoch #057, Train_Loss: [0.5366, 0.4724, 0.4076, 0.5180, 0.4570, 0.4827, 0.4618, 0.4791, 0.5765, 0.5201, 0.5418, 0.5189, 0.5091, 0.5134, 0.5796, 0.5277, 0.4920, 0.5182, 0.5089, 0.4925, 0.4938, 0.5435, 0.5334, 0.5804, 0.4666, 0.4765]
Val_Loss: 0.634851, Val_Acc: 0.710462

********** TEST START **********
Reload Best Model
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********
TEST :: Test_Acc: 0.644769
